{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4dd614-d7ee-47e1-9d5c-dcc24ec213a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 2.5.1+cu118\n",
      "torchvision version: 0.20.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "import wandb\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "print(f\"pytorch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad02288-6cf1-4f2f-ab65-02bb7edb1d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1edaa22-dadc-443e-8257-d50fdfec72ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int=42):\n",
    "    \"\"\" Sets random seed for torch operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int) : random seed to set\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac3b5b9-9316-4bc2-a438-f6ebea0ceff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../dataset/data')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"../dataset/\")\n",
    "image_path = data_path / \"data\"\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117ef78a-bc10-46f4-af65-29ef0b25f20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('../dataset/data/train_dir'),\n",
       " WindowsPath('../dataset/data/test_dir'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = image_path / \"train_dir\"\n",
    "test_dir = image_path / \"test_dir\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22875ba-4782-46fb-ac29-5d99733f6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\" Finds subdirectories in the given directory and maps class names to indices.\n",
    "\n",
    "    Args:\n",
    "        directory (str):  Path to the directory\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]\n",
    "        * A sorted list of class names\n",
    "        * A dictionary mapping class names to unique indices\n",
    "    \"\"\"\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9a8f1f1-bc6e-473e-bf72-6dd9a603f4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['daisy', 'dandelion', 'rose', 'sunflower', 'tulip'],\n",
       " {'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes, class_to_idx = find_classes(train_dir)\n",
    "classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4907dc68-a5ce-4d36-9c03-bb90341bae6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('../dataset/data/train_dir/daisy/100080576_f52e8ee070_n.jpg'),\n",
       " WindowsPath('../dataset/data/train_dir/daisy/10172379554_b296050f82_n.jpg'),\n",
       " WindowsPath('../dataset/data/train_dir/daisy/10172567486_2748826a8b.jpg'),\n",
       " WindowsPath('../dataset/data/train_dir/daisy/102841525_bd6628ae3c.jpg'),\n",
       " WindowsPath('../dataset/data/train_dir/daisy/10300722094_28fa978807_n.jpg'),\n",
       " WindowsPath('../dataset/data/train_dir/daisy/1031799732_e7f4008c03.jpg'),\n",
       " WindowsPath('../dataset/data/train_dir/daisy/10391248763_1d16681106_n.jpg'),\n",
       " WindowsPath('../dataset/data/train_dir/daisy/10437754174_22ec990b77_m.jpg'),\n",
       " WindowsPath('../dataset/data/train_dir/daisy/10437770546_8bb6f7bdd3_m.jpg'),\n",
       " WindowsPath('../dataset/data/train_dir/daisy/10437929963_bc13eebe0c.jpg')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = list(Path(train_dir).glob(\"*/*.jpg\"))\n",
    "paths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "738ae74f-602d-453d-b929-4c669b4b8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFolder(Dataset):\n",
    "    def __init__(self, target_dir: str, transform=None) -> None:\n",
    "        self.paths = list(Path(target_dir).glob(\"*/*.jpg\"))\n",
    "        self.transform = transform\n",
    "        self.classes, self.class_to_idx = find_classes(target_dir)\n",
    "\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img = self.load_image(index)\n",
    "        class_name = self.paths[index].parent.name\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        if self.transform:\n",
    "            return self.transform(img), class_idx\n",
    "        else:\n",
    "            return img, class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b619cc53-62e2-47a9-92cd-0391c916e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: aysenurciftcieee (aysenurciftci). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027e47b3fd0d4200bc506a572ee28cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011288888886984852, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\aysen\\OneDrive\\Masaüstü\\classification_flowers\\notebook\\wandb\\run-20241225_220922-f3nln2hj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aysenurciftci/pretained_vit_classification/runs/f3nln2hj' target=\"_blank\">dulcet-resonance-1</a></strong> to <a href='https://wandb.ai/aysenurciftci/pretained_vit_classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aysenurciftci/pretained_vit_classification' target=\"_blank\">https://wandb.ai/aysenurciftci/pretained_vit_classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aysenurciftci/pretained_vit_classification/runs/f3nln2hj' target=\"_blank\">https://wandb.ai/aysenurciftci/pretained_vit_classification/runs/f3nln2hj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aysenurciftci/pretained_vit_classification/runs/f3nln2hj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1c65e54cd30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    epochs=5,\n",
    "    classes=5,\n",
    "    batch_size=32,\n",
    "    learning_rate=1e-3,\n",
    " )\n",
    "\n",
    "wandb.init(project='pretained_vit_classification', entity='aysenurciftci', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4021807-ad26-4a03-8121-92b59f662a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_dir: str,\n",
    "                      test_dir: str,\n",
    "                      transform: transforms.Compose,\n",
    "                      batch_size: int):\n",
    "                        \n",
    "    \"\"\"Creates training and testing DataLoaders.\n",
    "\n",
    "    Args:\n",
    "        train_dir (str) : path to training directory\n",
    "        test_dir (str) : path to testing directory\n",
    "        transform : torchvision transforms to perform on training and testing data.\n",
    "        batch_size: Number of samples per batch\n",
    "        num_workers : An integer for number of workers per DataLoader.\n",
    "   \n",
    "    Returns:\n",
    "        A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "    \"\"\"\n",
    "\n",
    "    train_data = ImageFolder(train_dir, transform=transform)\n",
    "    test_data = ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    train_dataloader = DataLoader(train_data,\n",
    "                                  batch_size=batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  pin_memory=True)\n",
    "\n",
    "    test_dataloader = DataLoader(test_data,\n",
    "                                 batch_size=batch_size,\n",
    "                                 shuffle=True,\n",
    "                                 pin_memory=True)\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eed923a-3fdb-4c9d-a8d0-6a653252b4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_32-d86f8d99.pth\" to C:\\Users\\aysen/.cache\\torch\\hub\\checkpoints\\vit_b_32-d86f8d99.pth\n",
      "\n",
      "00%|███████████████████████████████████████████████████████████████████████████████| 337M/337M [01:12<00:00, 4.87MB/s]"
     ]
    }
   ],
   "source": [
    "pretrained_vit_weights = torchvision.models.ViT_B_32_Weights.DEFAULT\n",
    "\n",
    "pretrained_vit = torchvision.models.vit_b_32(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad=False\n",
    "\n",
    "#set_seed()\n",
    "pretrained_vit.heads = nn.Linear(in_features=768,\n",
    "                                 out_features=len(classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3873b6c-1309-4ee1-bdcd-8927c6e91fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "print(pretrained_vit_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "190e7a3f-3ba8-44a8-b838-3d226d0b029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=train_dir,\n",
    "                                                                   test_dir=test_dir,\n",
    "                                                                   transform=pretrained_vit_transforms,\n",
    "                                                                   batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37bf1836-3c23-4690-91f6-62b3b4e38777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 5]              768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 7, 7]      (2,360,064)          False\n",
       "├─Encoder (encoder)                                          [32, 50, 768]        [32, 50, 768]        38,400               False\n",
       "│    └─Dropout (dropout)                                     [32, 50, 768]        [32, 50, 768]        --                   --\n",
       "│    └─Sequential (layers)                                   [32, 50, 768]        [32, 50, 768]        --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [32, 50, 768]        [32, 50, 768]        (1,536)              False\n",
       "├─Linear (heads)                                             [32, 768]            [32, 5]              3,845                True\n",
       "============================================================================================================================================\n",
       "Total params: 87,459,077\n",
       "Trainable params: 3,845\n",
       "Non-trainable params: 87,455,232\n",
       "Total mult-adds (G): 5.52\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 845.22\n",
       "Params size (MB): 236.29\n",
       "Estimated Total Size (MB): 1100.77\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=pretrained_vit,\n",
    "        input_size=(32, 3, 224, 224),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdbd0163-4f8d-49d1-9912-9b308b7c3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              optimizer: torch.optim.Optimizer,\n",
    "              device: torch.device):\n",
    "\n",
    "    \"\"\" Perform a single training step for a pytorch model.\n",
    "\n",
    "    Args:\n",
    "        model : the neural network model to train.\n",
    "        dataloader : dataloader providing the training data in batches.\n",
    "        loss_fn : the loss function to evaluate the model's predictions.\n",
    "        optimizer : the optimizer to update the model's parameters.\n",
    "        device: target device\n",
    "    Returns:\n",
    "        tuple: \n",
    "            *train_loss (float) :  The average loss over the training set.\n",
    "            *train_acc (float) : The average accuracy over the training set.\n",
    "    \"\"\"\n",
    "\n",
    "    #put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        #send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        #forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        #calculate and acc metric\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9341eb4d-a68f-4353-a0c1-f344d1ee6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn : torch.nn.Module,\n",
    "              device: torch.device):\n",
    "\n",
    "    \"\"\" Performs a single evaluation step for a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model : the neural network model to evaluate.\n",
    "        dataloader : DataLoader providing the test/validation data in batches.\n",
    "        loss_fn : The loss function to evaluate the model's predictions.\n",
    "        device: the target device.\n",
    "\n",
    "    Returns: \n",
    "        tuple : \n",
    "            * test_loss (float) : The average loss over the test set.\n",
    "            * test_acc (float) : The average accuracy over the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    #put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            #send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            #forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "        test_loss = test_loss / len(dataloader)\n",
    "        test_acc = test_acc / len(dataloader)\n",
    "\n",
    "        return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22c63f77-a243-4e1d-b0a8-462bd99cf343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device):\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "    \"\"\"\n",
    "\n",
    "    results = {\"train_loss\" : [],\n",
    "              \"train_acc\" : [],\n",
    "              \"test_loss\" : [],\n",
    "              \"test_acc\": []\n",
    "              }\n",
    "\n",
    "    #loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn, \n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                       dataloader=test_dataloader,\n",
    "                                       loss_fn=loss_fn,\n",
    "                                       device=device)\n",
    "\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "          )\n",
    "\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_acc\": test_acc\n",
    "        })\n",
    "\n",
    "          # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "\n",
    "    wandb.save(\"model.onnx\")\n",
    "\n",
    "    return results   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b42930d-78e4-4adc-9f3f-4468939e5304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804a108927ec41bd89d6daf695b8e493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.5769 | train_acc: 0.8271 | test_loss: 0.3159 | test_acc: 0.9074\n",
      "Epoch: 2 | train_loss: 0.3101 | train_acc: 0.9111 | test_loss: 0.2564 | test_acc: 0.9190\n",
      "Epoch: 3 | train_loss: 0.2508 | train_acc: 0.9287 | test_loss: 0.2330 | test_acc: 0.9190\n",
      "Epoch: 4 | train_loss: 0.2167 | train_acc: 0.9398 | test_loss: 0.2203 | test_acc: 0.9213\n",
      "Epoch: 5 | train_loss: 0.1886 | train_acc: 0.9470 | test_loss: 0.2176 | test_acc: 0.9282\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
    "                             lr=1e-3)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "pretrained_vit_results = train(model=pretrained_vit,\n",
    "                              train_dataloader=train_dataloader,\n",
    "                              test_dataloader=test_dataloader,\n",
    "                              optimizer=optimizer,\n",
    "                              loss_fn=loss_fn,\n",
    "                              epochs=5,\n",
    "                              device=device,\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfb448-7911-4a67-a31d-9b1e0569ee3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a86382-5133-402b-8107-d83fda575014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ded89-3031-4a0e-a9f8-ed4700109310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
